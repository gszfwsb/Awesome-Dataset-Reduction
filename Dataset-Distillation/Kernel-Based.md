### üñºÔ∏è Kernel-based Methods

#### 2020

- **[1] Dataset meta-learning from kernel ridge-regression**, ICLR 2021.   
   *Timothy Nguyen, Zhourong Chen, Jaehoon Lee*   
  ![](https://img.shields.io/badge/KIP-blue) ![](https://img.shields.io/badge/Image_Classification-green)  ![](https://img.shields.io/badge/Kernel_Methods-red) ![](https://img.shields.io/badge/Dataset_Distillation-orange)
  <a href="https://openreview.net/forum?id=l-PrrQrK0QR"><img src="https://img.shields.io/badge/ICLR-Paper-%23D2691E?logo=ICLR" alt="Paper Badge"></a>
  <a href="https://github.com/google/neural-tangents"><img src="https://img.shields.io/badge/GitHub-Code-brightgreen?logo=github" alt="Code Badge"></a>

    <details> <summary>BibTex</summary>


    ```bibtex
        @inproceedings{
        nguyen2021dataset,
        title={Dataset Meta-Learning from Kernel Ridge-Regression},
        author={Timothy Nguyen and Zhourong Chen and Jaehoon Lee},
        booktitle={International Conference on Learning Representations},
        year={2021},
        url={https://openreview.net/forum?id=l-PrrQrK0QR}
    }

</details> 

#### 2021

- **[2] Dataset distillation with infinitely wide convolutional networks**, NeurIPS 2021.   
  *Timothy Nguyen, Roman Novak, Lechao Xiao, Jaehoon Lee*   
  ![](https://img.shields.io/badge/LS-blue) ![](https://img.shields.io/badge/Image_Classification-green)  ![](https://img.shields.io/badge/Kernel_Methods-red) ![](https://img.shields.io/badge/Dataset_Distillation-orange)
  <a href="https://papers.neurips.cc/paper/2021/file/299a23a2291e2126b91d54f3601ec162-Paper.pdf"><img src="https://img.shields.io/badge/NIPS-Paper-%23D2691E?logo=arXiv" alt="Paper Badge"></a>
  <a href="https://github.com/google/neural-tangents"><img src="https://img.shields.io/badge/GitHub-Code-brightgreen?logo=github" alt="Code Badge"></a>

    <details> <summary>BibTex</summary>


    ```bibtex
    @article{nguyen2021dataset,
      title={Dataset distillation with infinitely wide convolutional networks},
      author={Nguyen, Timothy and Novak, Roman and Xiao, Lechao and Lee, Jaehoon},
      journal={Advances in Neural Information Processing Systems},
      volume={34},
      pages={5186--5198},
      year={2021}
    }

</details> 

#### 2022

- **[3] Dataset distillation using neural feature regression**, NeurIPS 2022.   
  *Yongchao Zhou, Ehsan Nezhadarya, Jimmy Ba*   
  ![](https://img.shields.io/badge/FRePo-blue) ![](https://img.shields.io/badge/Image_Classification-green)  ![](https://img.shields.io/badge/Kernel_Methods-red) ![](https://img.shields.io/badge/Dataset_Distillation-orange)
  <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/3fe2a777282299ecb4f9e7ebb531f0ab-Supplemental-Conference.pdf"><img src="https://img.shields.io/badge/NIPS-Paper-%23D2691E?logo=arXiv" alt="Paper Badge"></a>
  <a href="https://github.com/yongchaoz/FRePo"><img src="https://img.shields.io/badge/GitHub-Code-brightgreen?logo=github" alt="Code Badge"></a>

    <details> <summary>BibTex</summary>


    ```bibtex
    @article{zhou2022dataset,
      title={Dataset distillation using neural feature regression},
      author={Zhou, Yongchao and Nezhadarya, Ehsan and Ba, Jimmy},
      journal={Advances in Neural Information Processing Systems},
      volume={35},
      pages={9813--9827},
      year={2022}
    }
    

</details> 

- **[4] Efficient dataset distillation using random feature approximation**, NeurIPS 2022.   
  *Noel Loo, Ramin Hasani, Alexander Amini, Daniela Rus*   
  ![](https://img.shields.io/badge/RFAD-blue)  ![](https://img.shields.io/badge/Image_Classification-green)  ![](https://img.shields.io/badge/Kernel_Methods-red) ![](https://img.shields.io/badge/Dataset_Distillation-orange)
  <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/5a28f46993c19f428f482cc59db40870-Paper-Conference.pdf"><img src="https://img.shields.io/badge/NIPS-Paper-%23D2691E?logo=arXiv" alt="Paper Badge"></a>
  <a href="https://github.com/yolky/RFAD"><img src="https://img.shields.io/badge/GitHub-Code-brightgreen?logo=github" alt="Code Badge"></a>
  
    <details> <summary>BibTex</summary>


    ```bibtex
    @article{loo2022efficient,
      title={Efficient dataset distillation using random feature approximation},
      author={Loo, Noel and Hasani, Ramin and Amini, Alexander and Rus, Daniela},
      journal={Advances in Neural Information Processing Systems},
      volume={35},
      pages={13877--13891},
      year={2022}
    }

</details> 

#### 2023

- **[5] Dataset distillation with convexified implicit gradients**, ICML 2023.   
  Noel Loo, Ramin Hasani, Mathias Lechner, Daniela Rus  
  ![](https://img.shields.io/badge/RCIG-blue) ![](https://img.shields.io/badge/Image_Classification-green)  ![](https://img.shields.io/badge/Kernel_Methods-red) ![](https://img.shields.io/badge/Dataset_Distillation-orange)
  <a href="https://proceedings.mlr.press/v202/loo23a/loo23a.pdf"><img src="https://img.shields.io/badge/ICML-Paper-%23D2691E?logo=arXiv" alt="Paper Badge"></a>
  <a href="https://github.com/yolky/RCIG"><img src="https://img.shields.io/badge/GitHub-Code-brightgreen?logo=github" alt="Code Badge"></a>

    <details> <summary>BibTex</summary>


    ```bibtex
    @inproceedings{loo2023dataset,
      title={Dataset distillation with convexified implicit gradients},
      author={Loo, Noel and Hasani, Ramin and Lechner, Mathias and Rus, Daniela},
      booktitle={International Conference on Machine Learning},
      pages={22649--22674},
      year={2023},
      organization={PMLR}
    }

</details> 

#### 2024

- **[6] Provable and Efficient Dataset Distillation for Kernel Ridge Regression**, NIPS 2024.   
  *Yu Yang, Hao Kang, Baharan Mirzasoleiman*   
  ![](https://img.shields.io/badge/KRR-blue) ![](https://img.shields.io/badge/Image_Classification-green)  ![](https://img.shields.io/badge/Kernel_Methods-red) ![](https://img.shields.io/badge/Dataset_Distillation-orange)
  <a href="https://openreview.net/pdf?id=WI2VpcBdnd"><img src="https://img.shields.io/badge/NIPS-Paper-%23D2691E?logo=arXiv" alt="Paper Badge"></a> <a href="https://github.com/Trustworthy-ML-Lab/provable-efficient-dataset-distill-KRR"><img src="https://img.shields.io/badge/GitHub-Code-brightgreen?logo=github" alt="Code Badge"></a>
  
    <details> <summary>BibTex</summary>


    ```bibtex
    @inproceedings{chenprovable,
      title={Provable and Efficient Dataset Distillation for Kernel Ridge Regression},
      author={Chen, Yilan and Huang, Wei and Weng, Tsui-Wei},
      booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems}
    }

</details> 



